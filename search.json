[
  {
    "objectID": "cluster-analysis/cancer-subtypes.html",
    "href": "cluster-analysis/cancer-subtypes.html",
    "title": "Cancer subtypes",
    "section": "",
    "text": "Classic paper\n\n\n\nvan ’t Veer L et al. Gene expression profiling predicts clinical outcome of breast cancer . Nature 415:530 (2002). See also the comment: The molecular outlook\n\n\n\n\nThe study by van ’t Veer et al was one of the first to use to microarrays, a brand-new technology at the time, to profile gene expression on a genome-wide scale from surgically removed tumour samples - breast tumours in this case. Another paper from around the same time is: Perou et al. Molecular portraits of human breast tumours. The credit for being the first to using cluster analysis on gene expression data (from yeast) probably goes to Eisen et al.’s Cluster analysis and display of genome-wide expression patterns.\nUsing cluster analysis to detect meaningful patterns in genome-scale data has stood the test of time and is often one of the first steps to take when exploring a new dataset.\n\n\n\nHow many tumour samples were analyzed by Van ’t Veer et al? How many genes were used for the cluster analysis and how were these genes selected\nThe most striking finding is in Figure 1. What does this figure show?\nClinical features to annotate and understand the observed separation of gene expression profiles in distinct clusters. What does each of the features measure and how do the authors characterize the overall classification of tumours? Starting points to read more about the clinical features:\n\nBRCA1 germline mutation: harmful variants in the BRCA1 or BRCA2 genes that markedly increase risk for developing breast cancer.\nEstrogen receptor (ER) status: breast tumour cells that express ER on their surface need estrogen to grow, and are therefore more susceptible to hormone therapy.\nTumour grade: a measure of degree of abnormality of cancer cells.\nLymphocyte infiltration: an indication whether the cancer has spread to the lymph nodes.\nAngionvasion: an indication whether the cancer has spread to the blood vessels.\nMetastatic status: an indication whether the cancer has spread to othre organs.\n\nThe authors identified a minimal prognostic signature from their data using a supervised approach. How does this approach work and how many marker genes were in the final, optimal set? For those with machine learning background, can you think of other (better?) supervised approaches to achieve the same goal?\n\n\n\nHaving identified a strong gene expression signature to predict clinical outcome of breast cancer, the race to bring it to the clinic is on. That this is far from trivial can be seen by tracing the follow-up studies and clinical trials:\n\nVan De Vijver MJ et al. A gene-expression signature as a predictor of survival in breast cancer. NEJM 347:1999 (2002).\nBuyse M et al. Validation and clinical utility of a 70-gene prognostic signature for women with node-negative breast cancer. J Ntnl Canc Inst 98:1183 (2006).\nMook S et al. Individualization of therapy using MammaPrint: From development to the MINDACT Trial. Canc Genomics & Proteomics 4:147 (2007).\nCardoso F et al. 70-gene signature as an aid to treatment decisions in early-stage breast cancer. NEJM 375:717 (2016).\nBrandão M, Pondé N, Piccart-Gebhart M. Mammaprint: a comprehensive review. Fut Onc 15:207 (2019).\n\nThey got there eventually, and the gene expression signature is now commercially available under the name of Mammaprint.",
    "crumbs": [
      "Cluster analysis",
      "Cancer subtypes"
    ]
  },
  {
    "objectID": "cluster-analysis/cancer-subtypes.html#gene-expression-profiling-predicts-clinical-outcome-of-breast-cancer",
    "href": "cluster-analysis/cancer-subtypes.html#gene-expression-profiling-predicts-clinical-outcome-of-breast-cancer",
    "title": "Cancer subtypes",
    "section": "",
    "text": "Classic paper\n\n\n\nvan ’t Veer L et al. Gene expression profiling predicts clinical outcome of breast cancer . Nature 415:530 (2002). See also the comment: The molecular outlook\n\n\n\n\nThe study by van ’t Veer et al was one of the first to use to microarrays, a brand-new technology at the time, to profile gene expression on a genome-wide scale from surgically removed tumour samples - breast tumours in this case. Another paper from around the same time is: Perou et al. Molecular portraits of human breast tumours. The credit for being the first to using cluster analysis on gene expression data (from yeast) probably goes to Eisen et al.’s Cluster analysis and display of genome-wide expression patterns.\nUsing cluster analysis to detect meaningful patterns in genome-scale data has stood the test of time and is often one of the first steps to take when exploring a new dataset.\n\n\n\nHow many tumour samples were analyzed by Van ’t Veer et al? How many genes were used for the cluster analysis and how were these genes selected\nThe most striking finding is in Figure 1. What does this figure show?\nClinical features to annotate and understand the observed separation of gene expression profiles in distinct clusters. What does each of the features measure and how do the authors characterize the overall classification of tumours? Starting points to read more about the clinical features:\n\nBRCA1 germline mutation: harmful variants in the BRCA1 or BRCA2 genes that markedly increase risk for developing breast cancer.\nEstrogen receptor (ER) status: breast tumour cells that express ER on their surface need estrogen to grow, and are therefore more susceptible to hormone therapy.\nTumour grade: a measure of degree of abnormality of cancer cells.\nLymphocyte infiltration: an indication whether the cancer has spread to the lymph nodes.\nAngionvasion: an indication whether the cancer has spread to the blood vessels.\nMetastatic status: an indication whether the cancer has spread to othre organs.\n\nThe authors identified a minimal prognostic signature from their data using a supervised approach. How does this approach work and how many marker genes were in the final, optimal set? For those with machine learning background, can you think of other (better?) supervised approaches to achieve the same goal?\n\n\n\nHaving identified a strong gene expression signature to predict clinical outcome of breast cancer, the race to bring it to the clinic is on. That this is far from trivial can be seen by tracing the follow-up studies and clinical trials:\n\nVan De Vijver MJ et al. A gene-expression signature as a predictor of survival in breast cancer. NEJM 347:1999 (2002).\nBuyse M et al. Validation and clinical utility of a 70-gene prognostic signature for women with node-negative breast cancer. J Ntnl Canc Inst 98:1183 (2006).\nMook S et al. Individualization of therapy using MammaPrint: From development to the MINDACT Trial. Canc Genomics & Proteomics 4:147 (2007).\nCardoso F et al. 70-gene signature as an aid to treatment decisions in early-stage breast cancer. NEJM 375:717 (2016).\nBrandão M, Pondé N, Piccart-Gebhart M. Mammaprint: a comprehensive review. Fut Onc 15:207 (2019).\n\nThey got there eventually, and the gene expression signature is now commercially available under the name of Mammaprint.",
    "crumbs": [
      "Cluster analysis",
      "Cancer subtypes"
    ]
  },
  {
    "objectID": "cluster-analysis/cancer-subtypes.html#the-cancer-genome-atlas",
    "href": "cluster-analysis/cancer-subtypes.html#the-cancer-genome-atlas",
    "title": "Cancer subtypes",
    "section": "The Cancer Genome Atlas",
    "text": "The Cancer Genome Atlas\nAlthough the results by Van ’t Veer et al. were obtained from a small (by current standards!) sample size, they have been reproduced consistenly in larger studies and arguably spawned a search for similar signatures in other cancer types through large-scale projects, such as The Cancer Genome Atlas (TCGA) Program.\nThe amount of data and number of publications produced by TCGA is too enormous to survey here in detail.\nIt is sometimes said that we humans overestimate the change that will happen in 5 years and underestimate the change that will happen in 10 years. To see how far the field progressed in the decade after van ’t Veer et al, read the 2012 TCGA paper on breast tumours.\n\nQuestions for discussion\nWhat are the main differences between van ’t Veer et al and the TCGA paper?\nHow is cluster analysis used in the TCGA paper?",
    "crumbs": [
      "Cluster analysis",
      "Cancer subtypes"
    ]
  },
  {
    "objectID": "cluster-analysis/combinatorial-clustering.html",
    "href": "cluster-analysis/combinatorial-clustering.html",
    "title": "Combinatorial clustering",
    "section": "",
    "text": "Book sections\n\n\n\n\nElements of Statistical Learning: Section 14.3\nAn Introduction to Statistical Learning: Section 12.4\n\n\n\n\nIntroduction\nThe goal of cluster analysis is to group or segment a collection of objects into subsets or “clusters”, such that objects within a cluster are more closely related than objects in different clusters.\nMany datasets exhibit a hierarchical structure, with no clear demarcation of clusters, and clusters can themselves be grouped successively such that clusters within one group are more similar than those in different groups. Deciding where to make the “cut” is usually done by setting parameters of a clustering algorithm, and almost always involves an arbitrary choice by the user.\nCentral to all clustering methods is the notion of the degree of similarity (or dissimilarity) between the objects being clustered. Sometimes data are presented directly in terms of proximity/similarity between objects. More often we have measurements (e.g. gene expression) on objects (e.g. genes or samples) that we want to cluster, and a (dis)similariy matrix must be constructed first.\n\n\nQuestions and problems for discussion\n\nDissimilarity measures\nAssume we have measurements \\(x_{ij}\\) for objects \\(i=1,2,\\dots,N\\) on variables (or attributes) \\(j=1,2,\\dots,p\\). Usually, we first define a dissimilarity function \\(d_j(x_{ij},x_{i'j})\\) between values of the \\(j\\)th attribute. How is the dissimilarity between objects defined and how can we vary the relative influence of a given attribute in the overall dissimilarity between objects?\n\n\n\n\n\n\nImportant\n\n\n\nTo give all attributes equal influence in the object dissimilarity, we must set their relative weights to \\(w_j\\sim 1/\\overline{d_j}\\), with\n\\[\n\\overline{d_j}=\\frac{1}{N^2} \\sum_{i=1}^N  \\sum_{i'=1}^N d_j(x_{ij},x_{i'j})\n\\]\n\n\n\n\n\n\n\n\nExercise\n\n\n\nSetting \\(w_j=1\\) for all \\(j\\) does not necessarily give all attributes equal influence! To see this, compute the average object dissimilarity over all pairs of objects. It should be a sum over attributes, and attributes have equal influence in the object dissimilarity if their contribution to this sum is one. Show that this results in the equation above.\n\n\nThe most common choice of dissimilarity function is squared-error distance:\n\\[\nd_j(x_{ij},x_{i'j}) = (x_{ij}-x_{i'j})^2\n\\]\nDefine the mean and variance of each attribute over all objects as\n\\[\n\\begin{aligned}\n\\mu_j &= \\frac1N \\sum_{i=1}^N x_{ij}\\\\\\\\\n\\sigma_j^2 &= \\frac1N \\sum_{i=1}^N (x_{ij}-\\mu_j)^2\n\\end{aligned}\n\\]\n\n\n\n\n\n\nExercise\n\n\n\nShow that with squared-error distance, the average object dissimilarity on the \\(j\\)th attribute is proportional to its variance.\n\n\nIt is often recommended to standardize data before clustering:\n\\[\nx_{ij} \\to y_{ij}=\\frac{x_{ij}-\\mu_j}{\\sigma_j}\n\\]\nWith squared-error loss, this is equivalent to setting weights \\(w_j \\sim 1/\\sigma_j^2 \\sim 1/\\bar{d}_j\\), that is, to give all attributes equal influence on the average object dissimilarity.\n\n\n\n\n\n\nImportant\n\n\n\nSometimes some attributes exhibit more grouping tendency than others, which may be obscured by standardizing. Find and understand the figure in the book sections above that illustrates this. The solution is to filter attributes by their variance before standardizing, and only use attributes with high variance for clustering. Why?\n\n\n\n\nCombinatorial clustering\nCombinatorial clustering algorithms assign each object to a cluster without regard to a probability model describing the data. Understanding combinatorial clustering is a necessary basis for understanding probabilistic methods.\nIn combinatorial clustering, a prespecified number of clusters \\(K&lt;N\\) is postulated (\\(N\\) the number of objects). An assignment of objects \\(i\\in\\{1,\\dots,N\\}\\) to clusters \\(k\\in\\{1,\\dots,K\\}\\) is charcterized by a many-to-one mapping or encoder \\(k=C(i)\\).\n\\(C\\) is obtained by minimizing the “within cluster” point scatter \\(W(C)\\), which characterizes the extent to which objects assigned to the same cluster tend to be close to one another. How is \\(W(C)\\) defined?\nWe can also define the “between cluster” point scatter \\(B(C)\\), which characterizes the extent to which objects assigned to different clusters tend to be far apart. How is \\(B(C)\\) defined?\nDoes it matter whether we find an optimal clustering by minimizing \\(W(C)\\) or by maximizing \\(B(C)\\)? Hint: compute \\(W(C)+B(C)\\).\n\n\nK-means clustering\nThe \\(K\\)-means algorithm uses the squared Euclidean distance \\[\nd(x_i,x_{i'}) = \\sum_{j=1}^p (x_{ij}-x_{i'j})^2 = \\\\\\| x_i - x_{i'}\\\\\\|^2\n\\] and an iterative greedy descent algorithm to minimize \\(W(C)\\).\nUsing the Euclidean distance expression, find an expression for \\(W(C)\\) in terms of the number of objects assigned to cluster \\(k\\) and the mean vector associated to cluster \\(k\\).\nShow that \\(W(C)\\) is minimized if within each cluster, the average dissimilarity of the objects from the cluster mean, as defined by the points in that cluster, is minimized.\nNote that for any set of objects \\(S\\), \\[\n\\overline{x_S} = \\frac{1}{|S|} \\sum_{i\\in S} x_i = \\text{argmin}_m \\sum_{i\\in S}\\\\\\|x_i-m\\\\\\|^2\n\\]\nFind out how this result is used in a greedy descent algorithm where alternatingly the mean vectors are updated for the current cluster assignments, and object assignments are updated by assigning objects to the nearest current mean vector.\n\n\nHow do we choose the number of clusters K?\nFind the recommended method in the book sections above.",
    "crumbs": [
      "Cluster analysis",
      "Combinatorial clustering"
    ]
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this website"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This is the website for the second half of the course BINF301 Genome-scale Algorithms. This part of the course focuses on machine learning algorithms for analyzing genome-scale data.\nMachine learning plays an important role in computational biology. See the Machine Learning in Computational and Systems Biology community or the Machine Learning in Computational Biology conference series.\nThe lectures are divided in modules, each focusing on a specific class of methods:\n\nClustering\nRegularized regression\nDimensionality reduction\nCausal inference\nGraphical models\nSpatio-Temporal models\n\nEach module follows the same structure:\n\nA classic or ground-breaking biological or biomedical research paper is studied where the algorithm (or class of algorithms) of interest was used.\nThe method used in the paper is studied in detail, along with additional methods to solve the same type of problem.\nThe methods are put in practice using, where possible, original data from the papers studied in the first part.\n\nThe computational demonstrations for the course were developed in Julia, and the course starts with an introduction to Julia. For the assignments, you can use any programming language.\nAn appendix contains the minimum required background knowledge on gene regulation, probability theory, linear algebra, and optimization.\nThe theoretical sections contain the basic information to understand a method, pointing to relevant sections of the following textbooks (with free pdfs!) for details:\n\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning (second edition) (2009).\nGareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, Jonathan Taylor. An Introduction to Statistical Learning (2023).\nChristopher Bishop. Pattern Recognition and Machine Learning (2006).\nMarc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for Machine Learning (2020).\n\nThe use of classic papers is motivated by Back to the future: education for systems-level biologists. Since the field of genome-scale data analysis is still relatively young, the choice of papers for study is still a bit open and likely to evolve as the course matures.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "julia-intro.html",
    "href": "julia-intro.html",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Julia is an open-source programming language that combines the interactivity of Python, R and Matlab, with the speed of C. Read more about its design principles and why it is good for scienticific applications, including computational biology here:\n\nWhy we created Julia\nJulia: come for the syntax, stay for the speed\nJulia for biologists\n\nInterestingly, despite not (yet!) being as popular as Python, ChatGPT performs better on Julia than Python (and R) for Large Language Model (LLM) code generation. Using AskAI, an interface to ChatGPT tailored specifically for Julia-related questions, should make Julia code generation even better.",
    "crumbs": [
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia-intro.html#why-julia",
    "href": "julia-intro.html#why-julia",
    "title": "Introduction to Julia",
    "section": "",
    "text": "Julia is an open-source programming language that combines the interactivity of Python, R and Matlab, with the speed of C. Read more about its design principles and why it is good for scienticific applications, including computational biology here:\n\nWhy we created Julia\nJulia: come for the syntax, stay for the speed\nJulia for biologists\n\nInterestingly, despite not (yet!) being as popular as Python, ChatGPT performs better on Julia than Python (and R) for Large Language Model (LLM) code generation. Using AskAI, an interface to ChatGPT tailored specifically for Julia-related questions, should make Julia code generation even better.",
    "crumbs": [
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia-intro.html#software-installation",
    "href": "julia-intro.html#software-installation",
    "title": "Introduction to Julia",
    "section": "Software installation",
    "text": "Software installation\nFollow the instructions on the MIT Introduction to Computational Thinking course to install Julia and Pluto.\nCreate an account on JuliaHub.",
    "crumbs": [
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "julia-intro.html#do-i-have-to-use-julia",
    "href": "julia-intro.html#do-i-have-to-use-julia",
    "title": "Introduction to Julia",
    "section": "Do I have to use Julia?",
    "text": "Do I have to use Julia?\nNo.\nAssignments can be submitted in any programming language.\nThe example notebooks are all in Julia, but their combination of text and code should (hopefully) make it easy to translate them to Jupyter, R markdown, or any other notebook/language combination of your choice. Likewise, it should not be too hard to replace JuliaHub by another cloud computing platform. If you do decide to reproduce the course notebooks in another language and/or on another platform, please share your results!",
    "crumbs": [
      "Introduction to Julia"
    ]
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "cluster-analysis/mixture-distributions.html",
    "href": "cluster-analysis/mixture-distributions.html",
    "title": "Mixture distributions",
    "section": "",
    "text": "Book sections\n\n\n\n\nPattern Recognition and Machine Learning: Chapter 9\nElements of Statistical Learning: Section 14.3\nAn Introduction to Statistical Learning: Section 12.4\n\n\n\n\nGenerative models\nA generative model is a statistical model for the process that could have generated your data. Generative models offer many advantages compared to combinatorial algorithms that treat data as a collection of objects. Most importantly, working with a generative model forces you to be explicit about your assumptions. Likewise, a generative model allows you to encode, and be explicit about, prior (biological) knowledge you may have about the data generating process.\n\n\nGaussian mixture models\nLet’s assume there is an unmeasured (or hidden) random variable \\(Z\\) that determines to which group an observation \\(X\\) belongs. For simplicity, assume that \\(Z\\) can only take two values, 0 or 1, and that the measurement \\(X\\) is one-dimensional and normally distributed in each group.\nConsider the following process:\n\nRandomly sample cluster label \\(Z=1\\) with probability \\(\\pi\\) and \\(Z=0\\) with probability \\(1-\\pi\\).\nSample features\n\n\\[\n\\begin{aligned}\nX \\sim \\begin{cases}\n\\mathcal{N}(\\mu_0,\\sigma_0^2) &  \\text{ if } Z=0\\\\\n\\mathcal{N}(\\mu_1,\\sigma_1^2) &  \\text{ if } Z=1\n\\end{cases}\n\\end{aligned}\n\\]\nTthe model generates cluster labels \\(Z\\) and real numbers \\(x\\in\\mathbb{R}\\) from the model\n\\[\n\\begin{aligned}\nZ &\\longrightarrow X\\\\\\\\\np(Z,x) = P(Z) &\\times p(x\\mid Z)\n\\end{aligned}\n\\]\nwhere we use lower-case “p” for probability density functions (\\(X\\) is continuous) and upper-case “P” for discrete probabilities, and\n\\[\n\\begin{aligned}\nP(Z=1) &= \\pi = 1 - P(Z=0) \\\\\\\\\np(x\\mid Z=k) &= \\mathcal{N}(\\mu_k,\\sigma_k^2)\n\\end{aligned}\n\\]\n\n\nSome EM language\nThe joint distribution is the probability distribution of a cluster label \\(Z\\) and feature value \\(x\\) both being produced by the model:\n\\[\np(Z,x) = p(x\\mid Z)\\\\; P(Z)\n\\]\nThe marginal distribution is th probability distribution that the model produces a feature value \\(x\\):\n\\[\np(x) = \\sum_{k=0,1} p(x\\mid Z=k)\\\\; P(Z=k)\n\\]\nThe responsibility of \\(Z\\) for feature value \\(x\\), also called the recognition distribution, is obtained using Bayes’ theorem\n\\[\nP(Z=k\\mid x) = \\frac{p(x\\mid Z=k) \\\\; P(Z=k)}{p(x)}\n\\]\nThis value can be used as a soft cluster assignment: with probability \\(P(Z=k \\mid x)\\), an observed value \\(x\\) belongs to cluster k.\nNote that the expected value of \\(Z\\) given a data point \\(x\\) is:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left(Z\\mid x\\right) &= 1 \\cdot P(Z=1 \\mid x) + 0\\cdot P(Z=0 \\mid x) = P(Z=1 \\mid x)\n% &= \\frac{\\pi p(x\\mid \\mu_1, \\igma_1^2)}{\\pi p(x \\mid \\mu_1, \\sigma_1^2) + (1-\\pi) p(x \\mid \\mu_0, \\sigma_0^2)}\n\\end{aligned}\n\\]\n\n\nMaximum-likelihood estimation\nTo fit the model to the data, we can only use the observed data \\(x\\), which follows the Gaussian mixture distribution\n\\[\np(x) = \\sum_{k=0,1} p(x\\mid Z=k)\\\\; P(Z=k)\n\\]\nThe log-likelihood of observing \\(N\\) independent samples \\((x_1,\\dots,x_N)\\) is\n\\[\n\\mathcal{L}= \\log\\left(\\prod_{i=1}^N p(x_{i}) \\right) = \\sum_{i=1}^N \\log p(x_{i})\n\\]\nWe want to find the best-fitting model by maximizing the log-likelihood.\nDirectly maximizing the log-likelihood with respect to the paramaters \\(\\pi\\), \\(\\mu_k\\), and \\(\\sigma_k^2\\) is difficult, because:\n\nOnly the feature values \\(x\\) are observed.\nThe cluster labels \\(Z\\) are hidden, they are latent variables.\nThe log-likelihood is expressed purely in terms of the observable distribution, involves logarithms of sums, and is intractable.\n\nIf we knew the cluster labels \\(k\\) for each sample, we could easily fit the parameters \\((\\pi,\\mu_k,\\sigma_k^2)\\) from the data for each cluster.\nIf we knew the parameters \\((\\pi, \\mu_k,\\sigma_k^2)\\), we could easily determine the probability for each data point to belong to each cluster and determine cluster labels.\nTo get around this catch-22, we replace actual cluster labels by their current expected values given current values for the parameters, and then iterate the above two steps until convergence - this is the Expectation-Maximization (EM) algorithm.\n\n\nThe EM algorithm\n\nTake initial guesses \\(\\hat\\pi\\), \\(\\hat\\mu_k\\), \\(\\hat\\sigma_k^2\\) for the model parameters.\nExpectation step: Compute the responsibilities \\(P(Z_i=k\\mid x_i)\\) for each data point \\(x_i\\).\nMaximization step: Update \\(\\hat\\pi\\), \\(\\hat\\mu_k\\), \\(\\hat\\sigma_k^2\\) by maximizing the log-likelihood using the soft cluster assignments \\(P(Z_i=k\\mid x_i)\\).\nIterate steps 2 and 3 until convergence.\n\n\n\nWhat are “soft cluster assignments”?\nConsider \\(N\\) samples \\((x_1,\\dots,x_N)\\) from a normal distribution \\(p(x\\mid \\mu,\\sigma^2)\\). The log-likelihood is\n\\[\n\\begin{aligned}\n\\mathcal{L}= \\sum_{i=1}^N \\log \\left(\\frac1{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2}\\right) = -\\frac{N}{2} \\log(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (x_i-\\mu)^2\n\\end{aligned}\n\\]\n\\(\\mathcal{L}\\) is maximized for\n\\[\n\\begin{aligned}\n\\hat\\mu &= \\frac{1}{N} \\sum_{i=1}^N x_i\\\\\\\\\n\\hat\\sigma^2 &= \\frac{1}{N} \\sum_{i=1}^N (x_i-\\hat\\mu)^2\n\\end{aligned}\n\\]\nNow consider \\(N\\) samples \\((Z_1,\\dots,Z_N)\\) and \\((x_1,\\dots,x_N)\\) from the generative model where the cluster labels are also observed. The log-likelihood is\n\\[\n\\begin{aligned}\n\\mathcal{L}&=  \\sum_{i=1}^N \\log p(Z_i,x_i)\\\\\\\\\n&= \\sum_{i=1}^N \\Bigl(Z_i \\log p(Z_i=1,x_i) + (1-Z_i) \\log p(Z_i=0,x_i)\\Bigr) \\\\\\\\\n&= \\sum_{i=1}^N \\Bigl(Z_i \\log p(x_i\\mid \\mu_1,\\sigma_1^2) + (1-Z_i) \\log p(x_i\\mid \\mu_0,\\sigma_0^2)\\Bigr) + \\sum_{i=1}^N \\Bigl(Z_i\\log\\pi + (1-Z_i) \\log(1-\\pi)\\Bigr)\n\\end{aligned}\n\\]\n\\(\\mathcal{L}\\) is maximized for\n\\[\n\\begin{aligned}\n\\hat\\pi &= \\frac{N_1}{N}\\\\\\\\\n\\hat\\mu_k &= \\frac{1}{N_k} \\sum_{Z_i=k} x_i\\\\\\\\\n\\hat\\sigma_k^2 &= \\frac{1}{N_k} \\sum_{Z_i=k}^N (x_i-\\hat\\mu_k)^2\n\\end{aligned}\n\\]\nSince the cluster labels are not observed, we don’t know the value of the \\(Z_i\\). The “trick” is to replace them with their expectation values \\(\\mathbb{E}(Z_i=k) = P(Z_i=k\\mid x_i)\\) in the EM algorithm, using the current estimates for \\(\\hat\\pi\\), \\(\\hat\\mu_k\\), \\(\\hat\\sigma_k^2\\).\nThis leads to updated estimates\n\\[\n\\begin{aligned}\n\\hat\\pi^{\\text{(new)}} &= \\frac{\\sum_{i=1}^N P(Z_i=k\\mid x_i)}{N}\\\\\\\\\n\\hat\\mu_k^{\\text{(new)}} &= \\frac{1}{N} \\sum_{i=1}^N P(Z_i=k\\mid x_i)\\\\;  x_i \\\\\\\\\n(\\hat\\sigma_k^2)^{\\text{(new)}} &= \\frac{1}{N} \\sum_{i=1}^N P(Z_i=k\\mid x_i)\\\\; (x_i-\\hat\\mu_k)^2\n\\end{aligned}\n\\]\nHence, instead of a “hard” assignment of each sample \\(x_i\\) to one cluster \\(k\\) when the \\(Z_i\\) are observed, each sample now contributes with a “soft assignment” weight \\(P(Z_i=k\\mid x_i)\\) to the parameters of each cluster.\nAfter convergence, the final \\(P(Z_i=k\\mid x_i)\\) can be used to assign data points to clusters, for instance to the cluster \\(k\\) with highest responsibility for \\(x_i\\).\n\n\nGeneralizations\nWe have so far considered the case where the data are one-dimensional (real numbers) and the number of clusters is pre-fixed. Important generalizations are:\n\nThe data can be of any dimension \\(D\\). In higher dimensions the components of the mixture model are multivariate normal distributions. The mean parameters \\(\\mu_k\\) simply become \\(D\\)-dimensional vectors. The variance parameters \\(\\sigma_k^2\\) however become \\(D\\times D\\) covariance matrices. For simplicity and to reduce the number of paramaters, it is often assumed that the covariance matrices are diagonal, such that the number of variance parameters again scales linearly in \\(D\\). However, when features are correlated, this assumption will be a poor representation of the underlying data.\nInstead of treating the cluster weights, means, and variances as fixed parameters that need to be estimated, they can be seen as random variables themselves with a distribution (uncertainty) that can be learned from the data. For more information, see this tutorial.\nIn an infinite mixture model, the number of clusters need not be fixed in advance, but can be learned from the data. For more information, see this tutorial.",
    "crumbs": [
      "Cluster analysis",
      "Mixture distributions"
    ]
  }
]